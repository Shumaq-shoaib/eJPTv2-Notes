# 1.1 Information Gathering

> :bookmark\_tabs: **Prerequisites**
>
> * Basic familiarity with Linux
> * Basic familiarity with web technologies
>
> **ðŸ“• Learning Objectives**
>
> * Differences between **active** and **passive** information gathering
> * Perform passive and active information gathering with various tools and resources

## What is Information Gathering?

* The initial phase of any penetration test involves information gathering. This step revolves around gathering data about an individual, company, website, or system that is the target of the assessment.
* Success in the later stages of a penetration test is closely linked to the extent of information gathered about the target. In other words, the more comprehensive the data collected, the higher the chances of success.
* Information gathering can be categorized into two main types: passive and active.

### **Passive Information Gathering**

It entails collecting data without directly engaging with the target.

* Identifying IP addresses & DNS information.
* Identifying domain names and domain ownership information.
* Identifying email addresses and social media profiles.
* Identifying web technologies being used on target sites.
* Identifying subdomains

### **Active Information Gathering**

It involves actively interacting with the target system, but it requires proper authorization before being performed.

* Discovering open ports on target systems.
* Learning about the internal infrastructure of a target network/organization.
* Enumerating information from target systems.

## Website Recon & Footprinting

**Reconnaissance**, aka "recon",  (often associated with information gathering) involves collecting information about a target system or organization using passive methods. This means gathering data without directly interacting with the target system, network, or organization.&#x20;

**Footprinting** is a specific phase of reconnaissance that focuses on gathering detailed information about a target system, network, or organization in order to create a "footprint" of its digital presence. This involves both passive and active methods to collect data about IP addresses, domain names, network infrastructure, organization structure, technical contacts, and more.

In a website, we try to seek information about:

* IP address,
* Directories hidden from search engines,
* Names,
* Email address,
* Phone numbers,
* Physical  Addresses,
* Web technologies being used.

### Whatis

The first thing that we can do is perform a **DNS lookup** by **whatis** command:

```
whatis website_url
```

to find information about website, for example if we have two address: it can means that website is behind a firewall/proxy.

Another good activity to do first to start is find website/**robotx.txt** file

The "**robots.txt**" file is a text file used by websites to communicate with web crawlers and search engine bots about which parts of the site should not be crawled or indexed. It is a standard used to provide instructions to web robots, also known as web crawlers or spiders, that automatically navigate and index websites for search engines like Google, Bing, and others.

Here are some key points about the "robots.txt" file:

1. **Purpose**: The main purpose of the "robots.txt" file is to control the behavior of web crawlers on a website. It allows website administrators to specify which parts of their site should not be crawled by search engine bots.
2. **Location**: The "robots.txt" file is typically placed in the root directory of a website. For example, if your website's domain is "example.com," the "robots.txt" file would be accessible at "[https://www.example.com/robots.txt](https://www.example.com/robots.txt)."
3. **Syntax**: The "robots.txt" file uses a simple syntax to specify user-agent (crawler) directives and disallow rules. User-agents are identifiers for specific web crawlers. Common user-agents include "Googlebot" for Google's crawler and "Bingbot" for Bing's crawler.
4. **Directives**:
   * **User-agent**: Specifies the web crawler to which the following rules apply.
   * **Disallow**: Instructs the specified user-agent not to crawl the specified URL paths.
   * **Allow**: Overrides a disallow rule for specific URL paths.
   * **Sitemap**: Specifies the location of the XML sitemap file that contains a list of URLs on the site.
5.  **Example**: Here's an example of a simple "robots.txt" file:

    ```javascript
    User-agent: *
    Disallow: /private/
    Disallow: /temp/
    Allow: /public/
    Sitemap: https://www.example.com/sitemap.xml
    ```

    In this example, the asterisk (\*) as the user-agent means the rules apply to all crawlers. It disallows crawling of the "/private/" and "/temp/" directories while allowing crawling of the "/public/" directory. It also specifies the location of the sitemap.

### XML Sitemap

Another good resource is **XML Sitemap** website/sitemap\_index.xml, that provide an organized way of indexing the website.

To identify website technology or content management system used in a website, we can use:

Add_-_ons:

* BuilWith,
* Wappalyzer,

_Cmd app:_

* WhatWeb

If we wanted to download the entire website and analyze the source code locally, we can use:

* [HTTrack](https://www.httrack.com/) Website Copier

### Whois

[**Whois**](https://who.is/) is a command and network protocol used to obtain detailed information about an internet domain name or an IP address. The term "whois" comes from the question "Who is?"

When you execute the "whois" command on a domain name or an IP address, you request information about the domain's owner, the registrar (the organization responsible for managing the domain registration), registration and expiration dates, technical and administrative contacts associated with the domain, and other related information.

For example, if you want to find out the registration details of a domain like "example.com," you can run the command "whois example.com" to retrieve relevant information about that domain.

### Netcraft

[Netcraft](https://www.netcraft.com/) is a website tool used to do website recon, in details can be used for::

**Website Analysis**: Netcraft offers tools and services to analyze websites. You can use their tools to gather information about a website's hosting provider, domain history, technology stack, and other related information.

**Security Research**: Netcraft monitors and reports on internet security threats, including phishing attacks, malware distribution, and other online scams. They maintain a database of phishing websites and offer browser extensions that help users detect and avoid potentially harmful websites.

**Server Surveys**: Netcraft conducts regular surveys to gather data about web servers and internet technologies. They track trends in web server software usage, operating systems, and other related statistics. This information can be valuable for businesses and researchers to understand the landscape of internet infrastructure.

**Domain and Hosting Information**: Netcraft provides tools to retrieve information about domain registrations, IP addresses, and hosting providers. This can be useful for investigative purposes or for businesses looking to gather competitive intelligence.

**Cybersecurity Services**: Netcraft's services include monitoring for DDoS attacks, phishing attempts, and other security threats. They offer services to help organizations identify and mitigate online threats to their websites and online services.

**Data Visualization**: Netcraft often presents its data and research through visualizations and reports. These can be helpful for understanding internet trends, security threats, and other relevant information.

**Historical Data**: Netcraft's historical data can be used to track changes in websites and online infrastructure over time. This can be useful for businesses looking to analyze their own or their competitors' online presence.

## DNS Reconnaissance

**DNS recon**,aka DNS reconnaissance, refers to the process of gathering information about domain names, their associated IP addresses, and other related domain records.&#x20;

**DNS** (Domain Name System) is the protocol used to translate human-readable domain names (like www.example.com) into IP addresses that computers use to communicate over the internet. DNS recon involves querying DNS servers to gather information about the target domain's DNS records. The information collected can provide valuable insights into the target's internet presence and infrastructure.

Here are some common aspects of DNS reconnaissance:

**DNS Records**: Different types of DNS records provide various types of information. For instance, an A record maps a domain to an IP address, an MX record specifies mail servers for email delivery, and a TXT record can hold arbitrary text data, often used for verification or DNS-based authentication.

**Subdomain Enumeration**: Subdomain enumeration is a common aspect of DNS recon. It involves discovering subdomains associated with a target domain. Attackers often use subdomain enumeration to identify potential entry points into a target's network.

**Reverse DNS Lookup**: Reverse DNS lookup involves querying the DNS system to find the domain name associated with a given IP address. This can help in identifying the domains hosted on a specific IP address.

**DNS Zone Transfers**: Zone transfers are a mechanism by which authoritative DNS servers share their DNS records with other authorized servers. Misconfigured DNS servers might allow unauthorized zone transfers, which can provide attackers with valuable information about the target's DNS infrastructure.

**Cache Snooping**: DNS caches store recently resolved DNS records to improve query response times. Cache snooping involves querying a DNS cache to retrieve information about recently resolved domain names. This can reveal information about internal network structure and previously visited websites.

### DNSRecon

[DNSRecon](https://www.kali.org/tools/dnsrecon/) is a Python script that provides the ability to perform:

* Check all NS Records for Zone Transfers.
* Enumerate General DNS Records for a given Domain (MX, SOA, NS, A, AAAA, SPF and TXT).
* Perform common SRV Record Enumeration.
* Top Level Domain (TLD) Expansion.
* Check for Wildcard Resolution.
* Brute Force subdomain and host A and AAAA records given a domain and a wordlist.
* Perform a PTR Record lookup for a given IP Range or CIDR.
* Check a DNS Server Cached records for A, AAAA and CNAME
* Records provided a list of host records in a text file to check.
* Enumerate Hosts and Subdomains using Google.

#### Command Examples

Scan a domain and save the results to a SQLite database:

```
# dnsrecon --domain example.com --db path/to/database.sqlite
```

Scan a domain, specifying the nameserver and performing a zone transfer:

```
# dnsrecon --domain example.com --name_server nameserver.example.com --type axfr
```

Scan a domain, using a brute-force attack and a dictionary of subdomains and hostnames:

```
# dnsrecon --domain example.com --dictionary path/to/dictionary.txt --type brt
```

Scan a domain, performing a reverse lookup of IP ranges from the SPF record and saving the results to a JSON file:

```
# dnsrecon --domain example.com -s --json
```

Scan a domain, performing a Google enumeration and saving the results to a CSV file:

```
# dnsrecon --domain example.com -g --csv
```

Scan a domain, performing DNS cache snooping:

```
# dnsrecon --domain example.com --type snoop --name_server nameserver.example.com --dictionary path/to/dictionary.txt
```

Scan a domain, performing zone walking:

```
# dnsrecon --domain example.com --type zonewalk
```

### DNSdumpster

[DNSdumpster](https://dnsdumpster.com/) is a domain research tool that can discover hosts related to a domain. Finding visible hosts from the attackers perspective is an important part of the security assessment process.

No brute force subdomain enumeration is used as is common in dns recon tools that enumerate subdomains. We use open source intelligence resources to query for related domain data. It is then compiled into an actionable resource for both attackers and defenders of Internet facing systems.

More than a simple DNS lookup this tool will discover those hard to find sub-domains and web hosts. The search relies on data from our crawls of the Alexa Top 1 Million sites, Search Engines, Common Crawl, Certificate Transparency, Max Mind, Team Cymru, Shodan and scans.io.



## WAF Recon

### **What is WAF?**

**WAF** stands for "Web Application Firewall." It is a security solution designed to protect web applications from a variety of online threats and attacks. Web applications are software applications that run on web servers and are accessible via web browsers. Examples of web applications include online stores, social media platforms, online banking sites, and more.

A Web Application Firewall works by analyzing incoming and outgoing HTTP/HTTPS requests to a web application. It aims to identify and mitigate various types of attacks that could exploit vulnerabilities in the application's code or configuration. Some of the common types of attacks that WAFs help prevent include:

* SQL Injection: Attackers attempt to insert malicious SQL statements into input fields to manipulate or extract data from a database.
* Cross-Site Scripting (XSS): Attackers inject malicious scripts into web pages viewed by other users, potentially leading to the theft of sensitive data or the hijacking of user sessions.
* Cross-Site Request Forgery (CSRF): Attackers trick users into performing actions on a web application without their consent or knowledge.
* Remote File Inclusion (RFI) and Local File Inclusion (LFI): Attackers attempt to include malicious files on the server to execute arbitrary code or access sensitive information.
* Brute Force Attacks: Attackers repeatedly try different username and password combinations to gain unauthorized access to an application.
* OWASP Top Ten Vulnerabilities: WAFs help address vulnerabilities listed in the OWASP Top Ten, a well-known list of the most critical web application security risks.

### Wafw00f

[**WAFW00F**](https://www.kali.org/tools/wafw00f/) stands for "Web Application Firewall Detection Tool." It is an open-source Python tool designed to identify and fingerprint Web Application Firewalls (WAFs) that are in use by a target website. A Web Application Firewall is a security tool designed to protect web applications from various online threats, such as SQL injection, cross-site scripting (XSS), and other types of attacks.

The scope of WAFW00F is to help security professionals, penetration testers, and web developers identify whether a target website is behind a WAF and potentially gather information about the specific WAF being used. This information can be valuable for assessing the security posture of a web application and understanding its defensive measures.

WAFW00F works by sending specially crafted HTTP requests to a target website and analyzing the responses. Based on patterns, headers, and behavior in the responses, it attempts to determine whether a WAF is in place and, if possible, provide information about the WAF vendor or technology. It can also help identify bypass techniques that attackers might use to evade detection by the WAF.

```bash
root@kali:~# wafw00f -h
Usage: wafw00f url1 [url2 [url3 ... ]]
example: wafw00f http://www.victim.org/

Options:
  -h, --help            show this help message and exit
  -v, --verbose         Enable verbosity, multiple -v options increase
                        verbosity
  -a, --findall         Find all WAFs which match the signatures, do not stop
                        testing on the first one
  -r, --noredirect      Do not follow redirections given by 3xx responses
  -t TEST, --test=TEST  Test for one specific WAF
  -o OUTPUT, --output=OUTPUT
                        Write output to csv, json or text file depending on
                        file extension. For stdout, specify - as filename.
  -f FORMAT, --format=FORMAT
                        Force output format to csv, json or text.
  -i INPUT, --input-file=INPUT
                        Read targets from a file. Input format can be csv,
                        json or text. For csv and json, a `url` column name or
                        element is required.
  -l, --list            List all WAFs that WAFW00F is able to detect
  -p PROXY, --proxy=PROXY
                        Use an HTTP proxy to perform requests, examples:
                        http://hostname:8080, socks5://hostname:1080,
                        http://user:pass@hostname:8080
  -V, --version         Print out the current version of WafW00f and exit.
  -H HEADERS, --headers=HEADERS
                        Pass custom headers via a text file to overwrite the
                        default header set.
```

## Subdomain Enumeration

### What is Subdomain?

A subdomain is a part of a larger domain in the Domain Name System (DNS) hierarchy. In a domain name like "example.com," the "example" is the main domain, and a subdomain would be a prefix added to it, such as "sub.example.com." Subdomains are used to organize and navigate different sections or services within a domain, and they allow for better management and segregation of content or functionality.

The scope of subdomains is to create a structured hierarchy within a domain and provide a way to categorize and manage various aspects of a website or online presence. Here are some common use cases and benefits of using subdomains:

* Content Organization: Subdomains can be used to organize different types of content or services. For example, a website might have "blog.example.com" for a blog section and "shop.example.com" for an online store.
* Geographical Segmentation: Companies with a global presence might use subdomains to serve localized content, like "us.example.com" for the U.S. and "uk.example.com" for the United Kingdom.
* Departmental Separation: Large organizations can use subdomains to separate different departments or teams. For instance, "hr.example.com" could be used for the Human Resources department.
* Testing and Development: Subdomains are often used for testing new features or development versions of a website. For example, "dev.example.com" might host a development version.
* Mobile and App Access: Subdomains can be used to provide access to mobile or app-specific content. "m.example.com" might be used for the mobile version of a site.
* API Endpoints: Subdomains can be dedicated to hosting APIs, such as "api.example.com," providing a clear separation of API resources from the main website.
* Tracking and Analytics: Subdomains can be used to set up tracking and analytics services separately from the main website, helping to manage data more effectively.
* Security and Isolation: Subdomains can offer a level of isolation between different parts of a website. If one subdomain is compromised, it might not automatically jeopardize other subdomains.

### Sublist3r

[**Sublist3r**](https://www.kali.org/tools/sublist3r/)&#x20;

is an open-source Python tool designed for subdomain discovery. Its primary purpose is to enumerate and gather information about subdomains associated with a particular domain. Subdomains are essentially sub-sections of a larger domain, and discovering them can be useful for various purposes, including security assessments, penetration testing, and gathering information about an organization's online presence.

The scope of Sublist3r includes:

* **Subdomain Enumeration**: Sublist3r aims to identify as many subdomains as possible associated with a given domain. This process involves querying different sources such as search engines, DNS records, and public databases to uncover subdomains.
* **Security Assessments**: Security professionals, ethical hackers, and penetration testers often use tools like Sublist3r to gather information about the subdomains of a target domain. This information can be valuable for identifying potential entry points for attacks, security vulnerabilities, or misconfigurations.
* **Red Team Operations**: In red teaming scenarios, where security teams simulate real-world attacks to test an organization's defenses, Sublist3r can be used to identify potential attack surfaces that adversaries might exploit.
* **Domain Footprinting**: Subdomain enumeration can help security researchers and analysts build a comprehensive profile of an organization's online presence, which can be useful for understanding an organization's attack surface and digital footprint.
* **Asset Discovery**: Organizations might use Sublist3r to discover subdomains associated with their domain to ensure they have visibility into all the resources and services available online.
* **Monitoring and Threat Intelligence**: Regularly monitoring subdomains can help organizations identify any unauthorized or potentially malicious subdomains that could be set up by attackers for phishing or other malicious purposes.

```bash
root@kali:~# sublist3r -h
usage: sublist3r [-h] -d DOMAIN [-b [BRUTEFORCE]] [-p PORTS] [-v [VERBOSE]]
                 [-t THREADS] [-e ENGINES] [-o OUTPUT] [-n]

OPTIONS:
  -h, --help            show this help message and exit
  -d DOMAIN, --domain DOMAIN
                        Domain name to enumerate it's subdomains
  -b [BRUTEFORCE], --bruteforce [BRUTEFORCE]
                        Enable the subbrute bruteforce module
  -p PORTS, --ports PORTS
                        Scan the found subdomains against specified tcp ports
  -v [VERBOSE], --verbose [VERBOSE]
                        Enable Verbosity and display results in realtime
  -t THREADS, --threads THREADS
                        Number of threads to use for subbrute bruteforce
  -e ENGINES, --engines ENGINES
                        Specify a comma-separated list of search engines
  -o OUTPUT, --output OUTPUT
                        Save the results to text file
  -n, --no-color        Output without color

Example: python3 /usr/bin/sublist3r -d google.com
```

## Google Dorks

Google Dorks, also known as Google Hacking or Google Search Operators, refer to specific search queries that leverage Google's advanced search operators to discover hidden or sensitive information on the internet. These operators allow users to refine their search queries and retrieve specific types of information that might not be readily available through standard searches. Google Dorks are often used by security professionals, hackers, and researchers to find vulnerabilities, exposed data, and potential security issues.

The scope of Google Dorks includes:

* **Information Gathering**: Google Dorks can be used to gather information about a target organization, including exposed files, directories, and subdomains. This information can be used by security professionals to assess potential risks and vulnerabilities.
* **Vulnerability Discovery**: Hackers and security researchers use Google Dorks to discover websites or web applications that might be misconfigured, vulnerable to attacks, or have exposed sensitive information.
* **Sensitive Data Disclosure**: Google Dorks can help uncover sensitive information like passwords, usernames, database credentials, and confidential documents that have been inadvertently exposed online.
* **Website Enumeration**: Attackers might use Google Dorks to enumerate directories and files on a website, identifying potential entry points for attacks or unauthorized access.
* **Exploitation**: If an attacker identifies a vulnerable website or application using Google Dorks, they might attempt to exploit the vulnerabilities they find.
* **Digital Footprint Analysis**: Organizations can use Google Dorks to assess their digital footprint and identify potential security gaps that could be exploited by malicious actors.
* **Phishing and Social Engineering**: Attackers might use Google Dorks to find email addresses, employee names, and other information to craft targeted phishing emails or conduct social engineering attacks.
* **Research and Education**: Security professionals and researchers can use Google Dorks to study online security issues, gather data for research, and understand potential risks and trends.

Here are a few examples of how Google Dorks can be used to search for specific types of information:

*   **Finding Open Directories**:

    ```vbnet
    intitle:"index of" "parent directory"
    ```

    This dork searches for directories that might not have an index file and could potentially expose the contents of the directory.
*   **Finding Exposed Documents**:

    ```makefile
    filetype:pdf site:example.com
    ```

    This dork searches for PDF files on a specific website domain. It can be used to find potentially sensitive documents that are publicly accessible.
*   **Identifying Vulnerable Servers**:

    ```vbnet
    intitle:"Apache HTTP Server Test Page" intext:"It works!"
    ```

    This dork looks for servers running the Apache HTTP Server with its default test page. It indicates that the server might be misconfigured or not fully secured.
*   **Discovering Login Pages**:

    ```makefile
    inurl:login site:example.com
    ```

    This dork searches for login pages within a specific website domain. It might help identify potential entry points for unauthorized access.
*   **Finding Publicly Exposed Files**:

    ```makefile
    site:example.com ext:sql
    ```

    This dork searches for SQL files on a specific website. It could potentially expose database files or SQL scripts.
*   **Identifying Security Cameras**:

    ```bash
    inurl:view/index.shtml
    ```

We can find more custom and pretty Google Dorks using query on [https://www.exploit-db.com/google-hacking-database](https://www.exploit-db.com/google-hacking-database).

<figure><img src="../../.gitbook/assets/Schermata del 2023-08-03 17-41-40.png" alt=""><figcaption><p><a href="https://www.exploit-db.com/google-hacking-database">https://www.exploit-db.com/google-hacking-database</a></p></figcaption></figure>

\






## Email Harvesting



Per far ciÃ², possiamo usare un comando linux di DNS lookup: host sitoweb

Una delle prime cose da fare Ã¨ ottenre l'iP address partendo dal sito web in nostro possesso, capendo se faccia uso o meno di un firewall/proxy server (cloudflare). Per es con host hackersploit.org avremo due IPv4 differenti, due IPv6 differenti e un email address



Una delle prime cose da fare Ã¨ controllare la presenza del file robots.txt





## DNS

The Domain Name System (DNS) is a protocol utilized to translate domain names or hostnames into their corresponding IP addresses.

Back in the early days of the internet, users needed to remember the numeric IP addresses of the websites they wanted to visit. However, DNS solves this issue by providing a mapping between easily recallable domain names and their respective IP addresses.

Think of a DNS server (nameserver) as a telephone directory, which holds a database of domain names and their corresponding IP addresses.

Numerous public DNS servers have been established by companies such as Cloudflare (1.1.1.1) and Google (8.8.8.8). These DNS servers contain records for almost all domains on the internet.

### DNS Records

* A - Resolves a hostname or domain to an IPv4 address.
* AAAA - Resolves a hostname or domain to an IPv6 address.
* NS - Reference to the domains nameserver.
* MX - Resolves a domain to a mail server.
* CNAME - Used for domain aliases.
* TXT - Text record.
* HINFO - Host information.
* SOA - Domain authority.
* SRV - Service records.
* PTR - Resolves an IP address to a hostname

### DNS Interrogation

DNS interrogation refers to the act of querying a DNS server to retrieve the DNS records associated with a particular domain.

The primary goal of DNS interrogation is to obtain relevant DNS records for the specified domain.

Through this process, valuable information such as the domain's IP address, subdomains, mail server addresses, and other related details can be obtained.

### DNS Zone Transfer

In certain cases DNS server admins may want to copy or transfer zone files from one DNS server to another. This process is known as a zone transfer.

* If misconfigured and left unsecured, this functionality can be abused by attackers to copy the zone file from the primary DNS server to another DNS server.
* A DNS Zone transfer can provide penetration testers with a holistic view of an organization's network layout.
* Furthermore, in certain cases, internal network addresses may be found on an organization's DNS servers.

