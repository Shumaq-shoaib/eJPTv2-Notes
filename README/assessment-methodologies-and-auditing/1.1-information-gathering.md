# 1.1 Information Gathering

> :bookmark\_tabs: **Prerequisites**
>
> * Basic familiarity with Linux
> * Basic familiarity with web technologies
>
> **ðŸ“• Learning Objectives**
>
> * Differences between **active** and **passive** information gathering
> * Perform passive and active information gathering with various tools and resources

## What is Information Gathering?

* The initial phase of any penetration test involves information gathering. This step revolves around gathering data about an individual, company, website, or system that is the target of the assessment.
* Success in the later stages of a penetration test is closely linked to the extent of information gathered about the target. In other words, the more comprehensive the data collected, the higher the chances of success.
* Information gathering can be categorized into two main types: passive and active.

### **Passive Information Gathering**

It entails collecting data without directly engaging with the target.

* Identifying IP addresses & DNS information.
* Identifying domain names and domain ownership information.
* Identifying email addresses and social media profiles.
* Identifying web technologies being used on target sites.
* Identifying subdomains

### **Active Information Gathering**

It involves actively interacting with the target system, but it requires proper authorization before being performed.

* Discovering open ports on target systems.
* Learning about the internal infrastructure of a target network/organization.
* Enumerating information from target systems.

## Website Recon & Footprinting

**Reconnaissance**, aka "recon",  (often associated with information gathering) involves collecting information about a target system or organization using passive methods. This means gathering data without directly interacting with the target system, network, or organization.&#x20;

**Footprinting** is a specific phase of reconnaissance that focuses on gathering detailed information about a target system, network, or organization in order to create a "footprint" of its digital presence. This involves both passive and active methods to collect data about IP addresses, domain names, network infrastructure, organization structure, technical contacts, and more.

In a website, we try to seek information about:

* IP address,
* Directories hidden from search engines,
* Names,
* Email address,
* Phone numbers,
* Physical  Addresses,
* Web technologies being used.

### Whatis

The first thing that we can do is perform a **DNS lookup** by **whatis** command:

```
whatis website_url
```

to find information about website, for example if we have two address: it can means that website is behind a firewall/proxy.

Another good activity to do first to start is find website/**robotx.txt** file

The "**robots.txt**" file is a text file used by websites to communicate with web crawlers and search engine bots about which parts of the site should not be crawled or indexed. It is a standard used to provide instructions to web robots, also known as web crawlers or spiders, that automatically navigate and index websites for search engines like Google, Bing, and others.

Here are some key points about the "robots.txt" file:

1. **Purpose**: The main purpose of the "robots.txt" file is to control the behavior of web crawlers on a website. It allows website administrators to specify which parts of their site should not be crawled by search engine bots.
2. **Location**: The "robots.txt" file is typically placed in the root directory of a website. For example, if your website's domain is "example.com," the "robots.txt" file would be accessible at "[https://www.example.com/robots.txt](https://www.example.com/robots.txt)."
3. **Syntax**: The "robots.txt" file uses a simple syntax to specify user-agent (crawler) directives and disallow rules. User-agents are identifiers for specific web crawlers. Common user-agents include "Googlebot" for Google's crawler and "Bingbot" for Bing's crawler.
4. **Directives**:
   * **User-agent**: Specifies the web crawler to which the following rules apply.
   * **Disallow**: Instructs the specified user-agent not to crawl the specified URL paths.
   * **Allow**: Overrides a disallow rule for specific URL paths.
   * **Sitemap**: Specifies the location of the XML sitemap file that contains a list of URLs on the site.
5.  **Example**: Here's an example of a simple "robots.txt" file:

    ```javascript
    User-agent: *
    Disallow: /private/
    Disallow: /temp/
    Allow: /public/
    Sitemap: https://www.example.com/sitemap.xml
    ```

    In this example, the asterisk (\*) as the user-agent means the rules apply to all crawlers. It disallows crawling of the "/private/" and "/temp/" directories while allowing crawling of the "/public/" directory. It also specifies the location of the sitemap.

### XML Sitemap

Another good resource is **XML Sitemap** website/sitemap\_index.xml, that provide an organized way of indexing the website.

To identify website technology or content management system used in a website, we can use:

Add_-_ons:

* BuilWith,
* Wappalyzer,

_Cmd app:_

* WhatWeb

If we wanted to download the entire website and analyze the source code locally, we can use:

* [HTTrack](https://www.httrack.com/) Website Copier

### Whois

[**Whois**](https://who.is/) is a command and network protocol used to obtain detailed information about an internet domain name or an IP address. The term "whois" comes from the question "Who is?"

When you execute the "whois" command on a domain name or an IP address, you request information about the domain's owner, the registrar (the organization responsible for managing the domain registration), registration and expiration dates, technical and administrative contacts associated with the domain, and other related information.

For example, if you want to find out the registration details of a domain like "example.com," you can run the command "whois example.com" to retrieve relevant information about that domain.

### Netcraft

[Netcraft](https://www.netcraft.com/) is a website tool used to do website recon, in details can be used for::

**Website Analysis**: Netcraft offers tools and services to analyze websites. You can use their tools to gather information about a website's hosting provider, domain history, technology stack, and other related information.

**Security Research**: Netcraft monitors and reports on internet security threats, including phishing attacks, malware distribution, and other online scams. They maintain a database of phishing websites and offer browser extensions that help users detect and avoid potentially harmful websites.

**Server Surveys**: Netcraft conducts regular surveys to gather data about web servers and internet technologies. They track trends in web server software usage, operating systems, and other related statistics. This information can be valuable for businesses and researchers to understand the landscape of internet infrastructure.

**Domain and Hosting Information**: Netcraft provides tools to retrieve information about domain registrations, IP addresses, and hosting providers. This can be useful for investigative purposes or for businesses looking to gather competitive intelligence.

**Cybersecurity Services**: Netcraft's services include monitoring for DDoS attacks, phishing attempts, and other security threats. They offer services to help organizations identify and mitigate online threats to their websites and online services.

**Data Visualization**: Netcraft often presents its data and research through visualizations and reports. These can be helpful for understanding internet trends, security threats, and other relevant information.

**Historical Data**: Netcraft's historical data can be used to track changes in websites and online infrastructure over time. This can be useful for businesses looking to analyze their own or their competitors' online presence.

## DNS Reconnaissance

**DNS recon**,aka DNS reconnaissance, refers to the process of gathering information about domain names, their associated IP addresses, and other related domain records.&#x20;

**DNS** (Domain Name System) is the protocol used to translate human-readable domain names (like www.example.com) into IP addresses that computers use to communicate over the internet. DNS recon involves querying DNS servers to gather information about the target domain's DNS records. The information collected can provide valuable insights into the target's internet presence and infrastructure.

Here are some common aspects of DNS reconnaissance:

**DNS Records**: Different types of DNS records provide various types of information. For instance, an A record maps a domain to an IP address, an MX record specifies mail servers for email delivery, and a TXT record can hold arbitrary text data, often used for verification or DNS-based authentication.

**Subdomain Enumeration**: Subdomain enumeration is a common aspect of DNS recon. It involves discovering subdomains associated with a target domain. Attackers often use subdomain enumeration to identify potential entry points into a target's network.

**Reverse DNS Lookup**: Reverse DNS lookup involves querying the DNS system to find the domain name associated with a given IP address. This can help in identifying the domains hosted on a specific IP address.

**DNS Zone Transfers**: Zone transfers are a mechanism by which authoritative DNS servers share their DNS records with other authorized servers. Misconfigured DNS servers might allow unauthorized zone transfers, which can provide attackers with valuable information about the target's DNS infrastructure.

**Cache Snooping**: DNS caches store recently resolved DNS records to improve query response times. Cache snooping involves querying a DNS cache to retrieve information about recently resolved domain names. This can reveal information about internal network structure and previously visited websites.

### DNSRecon

[DNSRecon](https://www.kali.org/tools/dnsrecon/) is a Python script that provides the ability to perform:

* Check all NS Records for Zone Transfers.
* Enumerate General DNS Records for a given Domain (MX, SOA, NS, A, AAAA, SPF and TXT).
* Perform common SRV Record Enumeration.
* Top Level Domain (TLD) Expansion.
* Check for Wildcard Resolution.
* Brute Force subdomain and host A and AAAA records given a domain and a wordlist.
* Perform a PTR Record lookup for a given IP Range or CIDR.
* Check a DNS Server Cached records for A, AAAA and CNAME
* Records provided a list of host records in a text file to check.
* Enumerate Hosts and Subdomains using Google.

#### Command Examples

Scan a domain and save the results to a SQLite database:

```
# dnsrecon --domain example.com --db path/to/database.sqlite
```

Scan a domain, specifying the nameserver and performing a zone transfer:

```
# dnsrecon --domain example.com --name_server nameserver.example.com --type axfr
```

Scan a domain, using a brute-force attack and a dictionary of subdomains and hostnames:

```
# dnsrecon --domain example.com --dictionary path/to/dictionary.txt --type brt
```

Scan a domain, performing a reverse lookup of IP ranges from the SPF record and saving the results to a JSON file:

```
# dnsrecon --domain example.com -s --json
```

Scan a domain, performing a Google enumeration and saving the results to a CSV file:

```
# dnsrecon --domain example.com -g --csv
```

Scan a domain, performing DNS cache snooping:

```
# dnsrecon --domain example.com --type snoop --name_server nameserver.example.com --dictionary path/to/dictionary.txt
```

Scan a domain, performing zone walking:

```
# dnsrecon --domain example.com --type zonewalk
```

### DNSdumpster

[DNSdumpster](https://dnsdumpster.com/) is a domain research tool that can discover hosts related to a domain. Finding visible hosts from the attackers perspective is an important part of the security assessment process.

No brute force subdomain enumeration is used as is common in dns recon tools that enumerate subdomains. We use open source intelligence resources to query for related domain data. It is then compiled into an actionable resource for both attackers and defenders of Internet facing systems.

More than a simple DNS lookup this tool will discover those hard to find sub-domains and web hosts. The search relies on data from our crawls of the Alexa Top 1 Million sites, Search Engines, Common Crawl, Certificate Transparency, Max Mind, Team Cymru, Shodan and scans.io.



## WAF Recon





## Subdomain Enumeration







## Google Dorks







## Email Harvesting



Per far ciÃ², possiamo usare un comando linux di DNS lookup: host sitoweb

Una delle prime cose da fare Ã¨ ottenre l'iP address partendo dal sito web in nostro possesso, capendo se faccia uso o meno di un firewall/proxy server (cloudflare). Per es con host hackersploit.org avremo due IPv4 differenti, due IPv6 differenti e un email address



Una delle prime cose da fare Ã¨ controllare la presenza del file robots.txt





## DNS

The Domain Name System (DNS) is a protocol utilized to translate domain names or hostnames into their corresponding IP addresses.

Back in the early days of the internet, users needed to remember the numeric IP addresses of the websites they wanted to visit. However, DNS solves this issue by providing a mapping between easily recallable domain names and their respective IP addresses.

Think of a DNS server (nameserver) as a telephone directory, which holds a database of domain names and their corresponding IP addresses.

Numerous public DNS servers have been established by companies such as Cloudflare (1.1.1.1) and Google (8.8.8.8). These DNS servers contain records for almost all domains on the internet.

### DNS Records

* A - Resolves a hostname or domain to an IPv4 address.
* AAAA - Resolves a hostname or domain to an IPv6 address.
* NS - Reference to the domains nameserver.
* MX - Resolves a domain to a mail server.
* CNAME - Used for domain aliases.
* TXT - Text record.
* HINFO - Host information.
* SOA - Domain authority.
* SRV - Service records.
* PTR - Resolves an IP address to a hostname

### DNS Interrogation

DNS interrogation refers to the act of querying a DNS server to retrieve the DNS records associated with a particular domain.

The primary goal of DNS interrogation is to obtain relevant DNS records for the specified domain.

Through this process, valuable information such as the domain's IP address, subdomains, mail server addresses, and other related details can be obtained.

### DNS Zone Transfer

In certain cases DNS server admins may want to copy or transfer zone files from one DNS server to another. This process is known as a zone transfer.

* If misconfigured and left unsecured, this functionality can be abused by attackers to copy the zone file from the primary DNS server to another DNS server.
* A DNS Zone transfer can provide penetration testers with a holistic view of an organization's network layout.
* Furthermore, in certain cases, internal network addresses may be found on an organization's DNS servers.

