# 1.1 Information Gathering

> :bookmark\_tabs: **Prerequisites**
>
> * Basic familiarity with Linux
> * Basic familiarity with web technologies
>
> **üìï Learning Objectives**
>
> * Differences between **active** and **passive** information gathering
> * Perform passive and active information gathering with various tools and resources

## What is Information Gathering?

* The initial phase of any penetration test involves information gathering. This step revolves around gathering data about an individual, company, website, or system that is the target of the assessment.
* Success in the later stages of a penetration test is closely linked to the extent of information gathered about the target. In other words, the more comprehensive the data collected, the higher the chances of success.
* Information gathering can be categorized into two main types: passive and active.

### **Passive Information Gathering**

It entails collecting data without directly engaging with the target.

* Identifying IP addresses & DNS information.
* Identifying domain names and domain ownership information.
* Identifying email addresses and social media profiles.
* Identifying web technologies being used on target sites.
* Identifying subdomains

### **Active Information Gathering**

It involves actively interacting with the target system, but it requires proper authorization before being performed.

* Discovering open ports on target systems.
* Learning about the internal infrastructure of a target network/organization.
* Enumerating information from target systems.

## Website Recon & Footprinting

**Reconnaissance**, aka "recon",  (often associated with information gathering) involves collecting information about a target system or organization using passive methods. This means gathering data without directly interacting with the target system, network, or organization.&#x20;

**Footprinting** is a specific phase of reconnaissance that focuses on gathering detailed information about a target system, network, or organization in order to create a "footprint" of its digital presence. This involves both passive and active methods to collect data about IP addresses, domain names, network infrastructure, organization structure, technical contacts, and more.

In a website, we try to seek information about:

* IP address,
* Directories hidden from search engines,
* Names,
* Email address,
* Phone numbers,
* Physical  Addresses,
* Web technologies being used.

### Whatis

The first thing that we can do is perform a **DNS lookup** by **whatis** command:

```
whatis website_url
```

to find information about website, for example if we have two address: it can means that website is behind a firewall/proxy.

Another good activity to do first to start is find website/**robotx.txt** file

The "**robots.txt**" file is a text file used by websites to communicate with web crawlers and search engine bots about which parts of the site should not be crawled or indexed. It is a standard used to provide instructions to web robots, also known as web crawlers or spiders, that automatically navigate and index websites for search engines like Google, Bing, and others.

Here are some key points about the "robots.txt" file:

1. **Purpose**: The main purpose of the "robots.txt" file is to control the behavior of web crawlers on a website. It allows website administrators to specify which parts of their site should not be crawled by search engine bots.
2. **Location**: The "robots.txt" file is typically placed in the root directory of a website. For example, if your website's domain is "example.com," the "robots.txt" file would be accessible at "[https://www.example.com/robots.txt](https://www.example.com/robots.txt)."
3. **Syntax**: The "robots.txt" file uses a simple syntax to specify user-agent (crawler) directives and disallow rules. User-agents are identifiers for specific web crawlers. Common user-agents include "Googlebot" for Google's crawler and "Bingbot" for Bing's crawler.
4. **Directives**:
   * **User-agent**: Specifies the web crawler to which the following rules apply.
   * **Disallow**: Instructs the specified user-agent not to crawl the specified URL paths.
   * **Allow**: Overrides a disallow rule for specific URL paths.
   * **Sitemap**: Specifies the location of the XML sitemap file that contains a list of URLs on the site.
5.  **Example**: Here's an example of a simple "robots.txt" file:

    ```javascript
    User-agent: *
    Disallow: /private/
    Disallow: /temp/
    Allow: /public/
    Sitemap: https://www.example.com/sitemap.xml
    ```

    In this example, the asterisk (\*) as the user-agent means the rules apply to all crawlers. It disallows crawling of the "/private/" and "/temp/" directories while allowing crawling of the "/public/" directory. It also specifies the location of the sitemap.

### XML Sitemap

Another good resource is **XML Sitemap** website/sitemap\_index.xml, that provide an organized way of indexing the website.

To identify website technology or content management system used in a website, we can use:

Add_-_ons:

* BuilWith,
* Wappalyzer,

_Cmd app:_

* WhatWeb

If we wanted to download the entire website and analyze the source code locally, we can use:

* [HTTrack](https://www.httrack.com/) Website Copier

### Whois

[**Whois**](https://who.is/) is a command and network protocol used to obtain detailed information about an internet domain name or an IP address. The term "whois" comes from the question "Who is?"

When you execute the "whois" command on a domain name or an IP address, you request information about the domain's owner, the registrar (the organization responsible for managing the domain registration), registration and expiration dates, technical and administrative contacts associated with the domain, and other related information.

For example, if you want to find out the registration details of a domain like "example.com," you can run the command "whois example.com" to retrieve relevant information about that domain.

### Netcraft







## DNS Recon





## WAF Recon





## Subdomain Enumeration







## Google Dorks







## Email Harvesting



Per far ci√≤, possiamo usare un comando linux di DNS lookup: host sitoweb

Una delle prime cose da fare √® ottenre l'iP address partendo dal sito web in nostro possesso, capendo se faccia uso o meno di un firewall/proxy server (cloudflare). Per es con host hackersploit.org avremo due IPv4 differenti, due IPv6 differenti e un email address



Una delle prime cose da fare √® controllare la presenza del file robots.txt





## DNS

The Domain Name System (DNS) is a protocol utilized to translate domain names or hostnames into their corresponding IP addresses.

Back in the early days of the internet, users needed to remember the numeric IP addresses of the websites they wanted to visit. However, DNS solves this issue by providing a mapping between easily recallable domain names and their respective IP addresses.

Think of a DNS server (nameserver) as a telephone directory, which holds a database of domain names and their corresponding IP addresses.

Numerous public DNS servers have been established by companies such as Cloudflare (1.1.1.1) and Google (8.8.8.8). These DNS servers contain records for almost all domains on the internet.

### DNS Records

* A - Resolves a hostname or domain to an IPv4 address.
* AAAA - Resolves a hostname or domain to an IPv6 address.
* NS - Reference to the domains nameserver.
* MX - Resolves a domain to a mail server.
* CNAME - Used for domain aliases.
* TXT - Text record.
* HINFO - Host information.
* SOA - Domain authority.
* SRV - Service records.
* PTR - Resolves an IP address to a hostname

### DNS Interrogation

DNS interrogation refers to the act of querying a DNS server to retrieve the DNS records associated with a particular domain.

The primary goal of DNS interrogation is to obtain relevant DNS records for the specified domain.

Through this process, valuable information such as the domain's IP address, subdomains, mail server addresses, and other related details can be obtained.

### DNS Zone Transfer

In certain cases DNS server admins may want to copy or transfer zone files from one DNS server to another. This process is known as a zone transfer.

* If misconfigured and left unsecured, this functionality can be abused by attackers to copy the zone file from the primary DNS server to another DNS server.
* A DNS Zone transfer can provide penetration testers with a holistic view of an organization's network layout.
* Furthermore, in certain cases, internal network addresses may be found on an organization's DNS servers.

